# @package _global_
defaults:
  - override /model: hstu

# ============================================================================
# 优化策略总结 (v2 - 修复 Embedding Collapse):
# ============================================================================
# 问题诊断: 模型发生了严重的 Embedding Collapse，表现为:
#   - Top-K 预测高度集中于少数几个 item (277107, 336221 等)
#   - 这些 item 的 embedding 余弦相似度高达 0.99+
#   - 它们的 embedding norm 异常大 (2.5+，而平均只有 0.3)
#
# 根本原因: 训练时正样本 embedding 未归一化，而负样本被归一化了
#   - 模型学到的是 "norm 越大越好" 而非真正的语义相似性
#   - 代码修复: models.py 中的 training_step 现在会对 supervision_embeddings 归一化
#
# 本配置的额外优化:
# 1. 降低学习率，防止快速坍塌
# 2. 增加 weight_decay，正则化 embedding norm
# 3. 降低温度到 0.1，增加任务难度（但不是 0.05 那么极端）
# 4. 增加负采样数量
# ============================================================================

model:
  optimizer:
    lr: 0.0005 # 降低学习率，防止快速坍塌
    weight_decay: 0.1 # 增加权重衰减，显著正则化 embedding norm

  configure_optimizer_params:
    monitor: val/ndcg@5
    mode: max

  # 增大 Embedding 维度以承载更多信息 (原 64)
  item_embedding_dim: 128

  # Preprocessor 优化
  preprocessor:
    # 适度 Dropout
    dropout_rate: 0.2

  # Encoder 优化 (HSTU)
  sequence_encoder:
    embedding_dim: ${model.item_embedding_dim}
    item_embedding_dim: ${model.item_embedding_dim}
    attention_dim: ${model.item_embedding_dim}
    linear_dim: ${model.item_embedding_dim}
    # 增加层数和头数，提升表达能力
    num_blocks: 4
    num_heads: 2
    # 适度正则化
    linear_dropout_rate: 0.2
    attn_dropout_rate: 0.1

  # 损失函数核心优化
  loss:
    _target_: tc_news_rec.models.losses.losses.SampledSoftmaxLoss
    # 负样本数量：36万物品，1024个负样本
    num_to_sample: 1024
    # 温度系数：0.1 比 0.05 更温和，但仍能关注难样本
    # 不要设为 1.0，那样任务太简单，loss 下降慢但不一定学得更好
    softmax_temperature: 0.1

# Trainer 优化
trainer:
  # 梯度裁剪，防止梯度爆炸
  gradient_clip_val: 1.0
  # 不使用梯度累积，保持简单
  accumulate_grad_batches: 1
  # 验证频率
  val_check_interval: 500
  log_every_n_steps: 50
  # 增加 epoch 数量，让模型充分学习
  max_epochs: 10
