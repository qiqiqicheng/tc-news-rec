# @package _global_
defaults:
  - override /model: hstu
  - override /data: large

# ============================================================================
# Experiment 2: InfoNCE + Slow Temperature Annealing
# ============================================================================
#
# Hypothesis: Slower annealing may help model learn better representations
# before transitioning to harder task.
#
# Changes from baseline:
# - Longer anneal_steps (20000 vs 10000)
# - Higher final_temperature (0.1 vs 0.05) for more stability
# - More epochs to accommodate slower learning
# ============================================================================

tags: ["infonce", "slow-annealing", "experiment"]

model:
  optimizer:
    lr: 0.0005
    weight_decay: 0.1

  configure_optimizer_params:
    monitor: val/mrr
    mode: max

  item_embedding_dim: 128

  preprocessor:
    dropout_rate: 0.2
    content_emb_mlp: true
    content_emb_hidden_dim: 512

  sequence_encoder:
    embedding_dim: ${model.item_embedding_dim}
    item_embedding_dim: ${model.item_embedding_dim}
    attention_dim: ${model.item_embedding_dim}
    linear_dim: ${model.item_embedding_dim}
    num_blocks: 4
    num_heads: 2
    linear_dropout_rate: 0.2
    attn_dropout_rate: 0.1

  negative_sampler:
    _target_: tc_news_rec.models.negative_samplers.negative_samplers.GlobalNegativeSampler
    l2_normalize: true

  loss:
    _target_: tc_news_rec.models.losses.losses.InfoNCELossWithAnnealing
    num_to_sample: 1024
    initial_temperature: 0.5
    final_temperature: 0.1      # Higher final temp for stability
    anneal_steps: 20000         # Slower annealing
    use_hard_negatives: false

data:
  sliding_window_augment: true
  sliding_window_step: 2
  sliding_window_min_len: 3
  batch_size: 256

trainer:
  gradient_clip_val: 1.0
  val_check_interval: 500
  log_every_n_steps: 50
  max_epochs: 20                # More epochs for slower learning
